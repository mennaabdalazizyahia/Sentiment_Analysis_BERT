# -*- coding: utf-8 -*-
"""sentiment_analysis_BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XFWlP2AUD788w172Fdk9vD5eZK5_ZWpf
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jp797498e/twitter-entity-sentiment-analysis")

print("Path to dataset files:", path)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data_training = pd.read_csv(path + '/twitter_training.csv',encoding='utf-8')
data_training.head()

data_val = pd.read_csv(path + '/twitter_validation.csv',encoding='utf-8')
data_val.head()

data_training.columns = ['id','entity','sentiment','comments']
data_val.columns = ['id','entity','sentiment','comments']

data_training

data_val

import tensorflow as tf
from tensorflow.keras import layers,models
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.preprocessing import LabelEncoder
from sentence_transformers import SentenceTransformer

data_training = data_training[['sentiment','comments']]
data_val = data_val[['sentiment','comments']]

data_training

data_training['sentiment'].value_counts()

sentiment_class = {'Negative': 'Negative','Positive': 'Positive','Neutral':'Neutral','Irrelevant':'Neutral'}

data_training['sentiment'] = data_training['sentiment'].map(sentiment_class)
data_val['sentiment'] = data_val['sentiment'].map(sentiment_class)

data_val['sentiment'].value_counts()

data_training.info()

data_training.isna().sum()

data_training = data_training.dropna(subset=['comments'])

data_training['comments'].isna().sum()

data_val.info()

data_val.isna().sum()

"""### BERT ###"""

bert_model = SentenceTransformer("all-MiniLM-L6-v2")

x_train = bert_model.encode(data_training['comments'].tolist(),batch_size=32,show_progress_bar=True)
x_val = bert_model.encode(data_val['comments'].tolist(),batch_size=32,show_progress_bar=True)

le = LabelEncoder()
y_train = le.fit_transform(data_training["sentiment"])
y_val = le.transform(data_val["sentiment"])

print(f'Classes :{le.classes_} ')

model = models.Sequential([
    layers.Input(shape=(x_train.shape[1],)),
    layers.Dense(256, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.3),
    layers.Dense(3, activation="softmax")
])

model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy']
)

model.summary()

history = model.fit(
    x_train,y_train,
    validation_data = (x_val,y_val),
    epochs = 10,
    batch_size = 32,
    verbose = 1
)

loss,accuracy = model.evaluate(x_val,y_val,verbose =0)
print(f'Accuracy:{accuracy : .2f} ')

y_pred_probs = model.predict(x_val)
y_pred = np.argmax(y_pred_probs, axis=1)
y_pred_labels = le.inverse_transform(y_pred)
y_true_labels = le.inverse_transform(y_val)

print(classification_report(y_true_labels, y_pred_labels))

cm = confusion_matrix(y_true_labels,y_pred_labels , labels = le.classes_)
plt.figure(figsize = (10,10))
sns.heatmap(cm,annot=True, fmt="d",
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labales')
plt.ylabel("True Lables")
plt.show()

labels = ['Positive','Negative','Neutral']
counts= [np.count_nonzero(y_pred_labels=='Positive'),
         np.count_nonzero(y_pred_labels =='Negative'),
         np.count_nonzero(y_pred_labels=='Neutral')]

plt.bar(labels,counts)
plt.title('Sentiment Distribution')
plt.show()

import joblib
model.save('sentiment_model.keras')
model.save('sentiment_model.h5')

joblib.dump(le,'label_encoder.joblib')

joblib.dump(bert_model,'sentence_encoder.joblib')

print('all models are saved')

