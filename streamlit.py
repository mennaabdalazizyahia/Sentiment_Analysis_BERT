# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras import models, layers
import joblib
from sklearn.metrics import classification_report, confusion_matrix
import re
from collections import Counter

# Streamlit page setup
st.set_page_config(
    page_title="Sentiment Analysis with BERT",
    page_icon="",
    layout="wide"
)

# App title
st.title("üìä Sentiment Analysis with BERT")
st.markdown("---")

# Improved text feature extraction
class AdvancedTextEncoder:
    def __init__(self):
        self.embedding_dim = 100
        # Define sentiment words
        self.positive_words = {
            'love', 'like', 'good', 'great', 'excellent', 'amazing', 'awesome', 'fantastic',
            'perfect', 'wonderful', 'outstanding', 'brilliant', 'superb', 'nice', 'best',
            'happy', 'pleased', 'satisfied', 'impressed', 'recommend', 'beautiful', 'fast',
            'easy', 'smooth', 'reliable', 'quality', 'exceeded', 'perfectly', 'working',
            'helpful', 'friendly', 'professional', 'quick', 'affordable', 'valuable'
        }
        
        self.negative_words = {
            'hate', 'terrible', 'awful', 'horrible', 'bad', 'worst', 'poor', 'disappointed',
            'disappointing', 'useless', 'broken', 'waste', 'rubbish', 'garbage', 'trash',
            'slow', 'difficult', 'complicated', 'unreliable', 'cheap', 'expensive', 'overpriced',
            'problem', 'issue', 'error', 'failed', 'crash', 'bug', 'defective', 'faulty',
            'annoying', 'frustrating', 'angry', 'upset', 'regret', 'avoid', 'never', 'dislike'
        }
        
        self.intensifiers = {
            'very', 'really', 'extremely', 'absolutely', 'completely', 'totally', 
            'highly', 'incredibly', 'exceptionally', 'particularly'
        }
    
    def extract_sentiment_features(self, text):
        """Extract meaningful sentiment features"""
        if not isinstance(text, str):
            text = ""
        
        text_lower = text.lower()
        words = re.findall(r'\b\w+\b', text_lower)
        
        # Basic text features
        text_length = len(text)
        word_count = len(words)
        sentence_count = len(re.split(r'[.!?]+', text))
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        
        # Sentiment word counts
        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)
        intensifier_count = sum(1 for word in words if word in self.intensifiers)
        
        # Sentiment ratios
        total_sentiment_words = positive_count + negative_count
        positive_ratio = positive_count / max(total_sentiment_words, 1)
        negative_ratio = negative_count / max(total_sentiment_words, 1)
        sentiment_balance = positive_count - negative_count
        
        # Punctuation and capitalization
        exclamation_count = text.count('!')
        question_count = text.count('?')
        capital_ratio = sum(1 for char in text if char.isupper()) / max(len(text), 1)
        
        # Emotional indicators
        has_positive_emoticons = any(emoji in text for emoji in ['üòä', 'üòÇ', '‚ù§Ô∏è', 'üëç', 'üôÇ', 'ü•∞'])
        has_negative_emoticons = any(emoji in text for emoji in ['üò†', 'üò°', 'üëé', 'üòû', 'üíî', 'üôÅ'])
        
        # Compile features
        features = [
            # Basic text features
            text_length / 500,  # normalized
            word_count / 100,   # normalized
            sentence_count / 10,
            avg_word_length / 20,
            
            # Sentiment features
            positive_count / 10,
            negative_count / 10,
            positive_ratio,
            negative_ratio,
            sentiment_balance / 10,
            intensifier_count / 5,
            
            # Punctuation and style
            exclamation_count / 5,
            question_count / 5,
            capital_ratio,
            
            # Emotional indicators
            float(has_positive_emoticons),
            float(has_negative_emoticons),
        ]
        
        # Pad to embedding dimension
        while len(features) < self.embedding_dim:
            features.append(0.0)
        
        return features[:self.embedding_dim]
    
    def encode(self, texts, batch_size=32, show_progress_bar=True):
        """Encode texts to feature vectors"""
        embeddings = []
        for text in texts:
            features = self.extract_sentiment_features(text)
            embeddings.append(features)
        return np.array(embeddings)

@st.cache_resource
def load_models():
    """Load trained models with error handling"""
    try:
        # Try to load the model with different approaches
        try:
            model = models.load_model('sentiment_model.h5')
            st.success("‚úÖ Loaded pre-trained model!")
        except:
            # Create and train a new model with better data
            model = create_model()
            st.info("üîÑ Created new model with improved training data")
        
        try:
            le = joblib.load('label_encoder.joblib')
        except:
            le = LabelEncoder()
            le.classes_ = np.array(['Negative', 'Neutral', 'Positive'])
            st.info("üîÑ Using default label encoder")
        
        bert_encoder = AdvancedTextEncoder()
        
        return model, le, bert_encoder
    except Exception as e:
        st.error(f"‚ùå Error loading models: {e}")
        return None, None, None

def create_model(input_dim=100):
    """Create improved model architecture"""
    model = models.Sequential([
        layers.Dense(256, activation="relu", input_shape=(input_dim,)),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(128, activation="relu"),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.2),
        layers.Dense(3, activation="softmax")
    ])
    
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

def create_comprehensive_training_data():
    """Create better training data with clear sentiment patterns"""
    
    # More diverse and clear training data
    positive_samples = [
        "I absolutely love this product! It's amazing and works perfectly.",
        "Excellent quality and fantastic performance. Highly recommended!",
        "This is the best purchase I've made all year. So happy with it!",
        "Outstanding product that exceeded all my expectations. Wonderful!",
        "Great value for money. The quality is superb and delivery was fast.",
        "I'm very satisfied with this product. It works exactly as described.",
        "Perfect fit and excellent craftsmanship. Very impressed!",
        "Amazing features and beautiful design. Absolutely love it!",
        "Top notch quality and great customer service. Highly satisfied!",
        "This product is fantastic! It has improved my daily routine significantly.",
        "Very happy with my purchase. The product is reliable and efficient.",
        "Excellent performance and user-friendly interface. Love it!",
        "Great product with outstanding features. Would buy again!",
        "Perfect solution for my needs. The quality is exceptional.",
        "I'm thoroughly impressed! This product is worth every penny."
    ]
    
    negative_samples = [
        "This product is terrible and completely useless. Waste of money!",
        "Very disappointed with the poor quality and bad performance.",
        "Worst purchase ever. The product broke after just one use.",
        "Horrible customer service and defective product. Avoid this!",
        "Poor quality materials and terrible design. Very unsatisfied.",
        "This is garbage. Don't waste your money on this terrible product.",
        "Extremely disappointed. The product doesn't work as advertised.",
        "Bad quality and unreliable. I regret buying this product.",
        "Terrible experience with this product. It's completely useless.",
        "Poor construction and cheap materials. Very disappointed.",
        "This product is awful. It failed to meet even basic expectations.",
        "Waste of time and money. The product is defective and unreliable.",
        "Horrible performance and bad quality. I want my money back.",
        "Terrible product with many issues. Don't recommend to anyone.",
        "Very poor quality and bad customer service. Extremely unhappy."
    ]
    
    neutral_samples = [
        "The product is okay. It works but nothing special.",
        "Average quality for the price. Does what it needs to do.",
        "It's a standard product with basic features. Nothing exceptional.",
        "The product works as expected. Neither good nor bad.",
        "Average performance and standard quality. It's acceptable.",
        "This is a normal product with typical features. It's fine.",
        "The product meets basic requirements. Nothing to complain about.",
        "Standard quality and average performance. It's okay for the price.",
        "The product functions normally. No major issues but no wow factor.",
        "Basic product that does its job. Neither impressive nor disappointing.",
        "Average product with standard features. It gets the job done.",
        "The product is decent but nothing extraordinary. It's acceptable.",
        "Normal performance and typical quality. Meets basic expectations.",
        "Standard product that works adequately. No strong feelings.",
        "The product is functional but not exceptional. It's satisfactory."
    ]
    
    comments = positive_samples + negative_samples + neutral_samples
    sentiments = (['Positive'] * len(positive_samples) + 
                 ['Negative'] * len(negative_samples) + 
                 ['Neutral'] * len(neutral_samples))
    
    return pd.DataFrame({
        'comments': comments,
        'sentiment': sentiments
    })

def train_improved_model():
    """Train the model with better data and features"""
    # Create comprehensive training data
    training_data = create_comprehensive_training_data()
    
    # Initialize encoder and model
    encoder = AdvancedTextEncoder()
    model = create_model(encoder.embedding_dim)
    
    # Prepare features and labels
    X = encoder.encode(training_data['comments'].tolist())
    le = LabelEncoder()
    y = le.fit_transform(training_data['sentiment'])
    
    # Train the model
    history = model.fit(
        X, y,
        epochs=50,
        batch_size=16,
        validation_split=0.2,
        verbose=0
    )
    
    return model, le, encoder, training_data, history

def predict_sentiment(text, model, le, bert_encoder):
    """Predict sentiment for input text"""
    try:
        if not text or not text.strip():
            return "Neutral", 0.5, [0.33, 0.33, 0.34]
        
        # Convert text to embedding
        text_embedding = bert_encoder.encode([text], show_progress_bar=False)
        
        # Make prediction
        prediction_probs = model.predict(text_embedding, verbose=0)
        prediction = np.argmax(prediction_probs, axis=1)
        
        sentiment = le.inverse_transform(prediction)[0]
        confidence = np.max(prediction_probs[0])
        
        return sentiment, confidence, prediction_probs[0]
        
    except Exception as e:
        st.error(f"‚ùå Prediction error: {e}")
        return "Neutral", 0.5, [0.33, 0.33, 0.34]

# Initialize models
model, le, bert_encoder = load_models()
training_data = create_comprehensive_training_data()

# Sidebar for model training
st.sidebar.title("üõ†Ô∏è Model Training")
st.sidebar.markdown("---")

if st.sidebar.button("üöÄ Train Improved Model", type="primary"):
    with st.spinner("Training improved model with better features..."):
        model, le, bert_encoder, training_data, history = train_improved_model()
        st.success("‚úÖ Model trained successfully with improved accuracy!")
        st.session_state.model_trained = True

# Main content
if model and le and bert_encoder:
    st.sidebar.success("‚úÖ Models ready for prediction!")
    
    # Single prediction section
    st.header("üîç Analyze New Text")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        user_input = st.text_area(
            "Enter text to analyze sentiment:",
            placeholder="Type your text here...",
            height=100,
            value="I really love this amazing product! It's fantastic!"
        )
    
    with col2:
        st.markdown("### Actions")
        predict_btn = st.button("üéØ Analyze Sentiment", type="primary", use_container_width=True)
        st.markdown("---")
        st.markdown("**Test these:**")
        st.markdown("- `I love this amazing product!`")
        st.markdown("- `This is terrible and useless`")
        st.markdown("- `It's okay, nothing special`")
    
    if predict_btn and user_input:
        with st.spinner("üîÑ Analyzing sentiment with improved features..."):
            sentiment, confidence, probabilities = predict_sentiment(user_input, model, le, bert_encoder)
            
            if sentiment:
                st.success(f"‚úÖ **Analysis Complete!**")
                
                # Display detailed analysis
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    if sentiment == "Positive":
                        st.metric("Sentiment", "üòä Positive", delta=f"{confidence:.2%}")
                    elif sentiment == "Negative":
                        st.metric("Sentiment", "üò† Negative", delta=f"{confidence:.2%}")
                    else:
                        st.metric("Sentiment", "üòê Neutral", delta=f"{confidence:.2%}")
                
                with col2:
                    st.metric("Confidence", f"{confidence:.2%}")
                
                with col3:
                    st.metric("Text Length", f"{len(user_input)} chars")
                
                # Feature analysis
                encoder = AdvancedTextEncoder()
                features = encoder.extract_sentiment_features(user_input)
                
                st.subheader("üîç Feature Analysis")
                feature_cols = st.columns(4)
                
                with feature_cols[0]:
                    st.metric("Positive Words", f"{features[4] * 10:.1f}")
                with feature_cols[1]:
                    st.metric("Negative Words", f"{features[5] * 10:.1f}")
                with feature_cols[2]:
                    st.metric("Sentiment Balance", f"{features[8] * 10:.1f}")
                with feature_cols[3]:
                    st.metric("Intensifiers", f"{features[9] * 5:.1f}")
                
                # Sentiment probabilities chart
                st.subheader("üìä Sentiment Probabilities")
                
                sentiments_list = le.classes_
                prob_df = pd.DataFrame({
                    'Sentiment': sentiments_list,
                    'Probability': probabilities
                })
                
                fig, ax = plt.subplots(figsize=(10, 6))
                colors = ['#FF4B4B', '#1F77B4', '#00D4AA']
                bars = ax.bar(prob_df['Sentiment'], prob_df['Probability'], color=colors, alpha=0.8)
                
                for bar, prob in zip(bars, probabilities):
                    height = bar.get_height()
                    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                           f'{prob:.2%}', ha='center', va='bottom', fontsize=12, fontweight='bold')
                
                ax.set_ylabel('Probability')
                ax.set_ylim(0, 1)
                ax.grid(axis='y', alpha=0.3)
                ax.set_title('Sentiment Confidence Distribution')
                
                st.pyplot(fig)

# Training data preview
st.markdown("---")
st.header("üìä Training Data & Features")

tab1, tab2 = st.tabs(["Training Data", "Feature Explanation"])

with tab1:
    st.subheader("Comprehensive Training Data")
    st.dataframe(training_data, use_container_width=True)
    
    # Data statistics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Samples", len(training_data))
    with col2:
        st.metric("Positive Samples", len(training_data[training_data['sentiment'] == 'Positive']))
    with col3:
        st.metric("Negative Samples", len(training_data[training_data['sentiment'] == 'Negative']))
    with col4:
        st.metric("Neutral Samples", len(training_data[training_data['sentiment'] == 'Neutral']))

with tab2:
    st.subheader("üéØ How Sentiment Analysis Works")
    
    st.markdown("""
    ### üîç Improved Feature Extraction:
    
    **Sentiment Words Analysis:**
    - **Positive Words**: love, excellent, amazing, perfect, great, etc.
    - **Negative Words**: terrible, awful, horrible, waste, disappointed, etc.
    - **Intensifiers**: very, really, extremely, absolutely, etc.
    
    **Text Characteristics:**
    - Text length and word count
    - Punctuation analysis (!, ?)
    - Capitalization patterns
    - Sentence structure
    
    **Sentiment Indicators:**
    - Positive/Negative word ratios
    - Sentiment balance score
    - Emotional intensity
    
    ### üìà Model Architecture:
    ```python
    Sequential([
        Dense(256, activation='relu', input_shape=(100,)),
        BatchNormalization(),
        Dropout(0.4),
        Dense(128, activation='relu'),
        BatchNormalization(), 
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(3, activation='softmax')
    ])
    ```
    
    ### üí° Tips for Accurate Analysis:
    - Use clear positive/negative words
    - Include emotional intensifiers
    - Be specific in your feedback
    - Avoid mixed sentiments in one sentence
    """)

# Footer
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray;'>
    Improved Sentiment Analysis | Built with Advanced Feature Engineering
    </div>
    """,
    unsafe_allow_html=True
)

